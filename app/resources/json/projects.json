[
    {
        "title": "Flat File Ingestion into Google Bigquery Tables",
        "description": "<p><strong>Project Overview:</strong>Data Ingestion and Management with Google Cloud Composer and Airflow\n\n<h3>Overview</h3>\n<p>This project entailed the development and implementation of a data ingestion pipeline using Python-based Directed Acyclic Graphs (DAGs) in Google Cloud Composer, a fully managed workflow orchestration service built on Apache Airflow. The primary goal was to efficiently ingest flat files from Google Cloud Storage buckets into BigQuery, leveraging an in-house <code>bqutils</code> ingest application.</p>\n\n<h3>Responsibilities</h3>\n\n<h4>Table Creation and Management</h4>\n<ul>\n  <li><strong>Design and Implementation</strong>: Developed the schema for required data tables in BigQuery, ensuring optimal structure for efficient data storage and retrieval.</li>\n  <li><strong>DDL (Data Definition Language) SQLs</strong>: Authored SQL scripts for creating, altering, and managing database objects, ensuring the integrity and efficiency of the data structure.</li>\n</ul>\n\n<h4>DAGs Development</h4>\n<ul>\n  <li><strong>Workflow Design</strong>: Created and tested Python-based DAGs in Google Cloud Composer, orchestrating the sequence of tasks for data ingestion.</li>\n  <li><strong>Automation and Scheduling</strong>: Configured DAGs to automate the workflow, scheduling the ingestion process to meet project requirements and timelines.</li>\n</ul>\n\n<h4>Data Preprocessing</h4>\n<ul>\n  <li><strong>SQL-Based Processing</strong>: Executed complex SQL queries for data transformation and preprocessing, enhancing data quality and usability for downstream applications.</li>\n  <li><strong>Data Validation</strong>: Ensured the accuracy and completeness of the data through rigorous validation processes.</li>\n</ul>\n\n<h4>DML (Data Manipulation Language) SQLs</h4>\n<ul>\n  <li><strong>Query Development</strong>: Authored and optimized SQL scripts for data manipulation, including insertion, deletion, and modification of data within BigQuery.</li>\n</ul>\n\n<h4>Collaboration and Coordination</h4>\n<ul>\n  <li><strong>Teamwork</strong>: Collaborated closely with cross-functional teams to align the data ingestion pipeline with overall project objectives.</li>\n  <li><strong>Problem-Solving</strong>: Addressed and resolved technical challenges, optimizing the workflow for efficiency and reliability.</li>\n</ul>\n\n<h3>Technologies Used</h3>\n<ul>\n  <li><strong>Google Cloud Composer</strong>: For workflow orchestration and DAGs management.</li>\n  <li><strong>Apache Airflow</strong>: Utilized within Google Cloud Composer for creating and scheduling workflows.</li>\n  <li><strong>Google Cloud Storage</strong>: As the source for flat file data storage.</li>\n  <li><strong>BigQuery</strong>: For data warehousing and executing SQL queries.</li>\n  <li><strong>Python</strong>: For scripting DAGs and automating the data ingestion process.</li>\n  <li><strong>bqutils</strong>: In-house application for streamlined data ingestion into BigQuery.</li>\n</ul>\n\n<h3>Project Outcome</h3>\n<p>The project successfully automated the process of ingesting flat files from Google Cloud Storage into BigQuery, \nstreamlining the data management pipeline. This led to significant improvements in data processing efficiency and \nreliability, supporting the organization's data-driven decision-making capabilities.</p>\n",
        "github_link": "N/A"
    },
    {
        "title": "Cloud Migration Project: On-Premises to Google Cloud",
        "description": "<p><strong>Project Overview:</strong> Leading the migration of on-premises databases to Google Cloud, ensuring efficient data transfer and management.</p>\n    <h4>Key Responsibilities</h4>\n    <ul>\n        <li>Utilized <em>Infoworks</em> for building and scheduling pipelines, facilitating data transfer from diverse sources to the cloud.</li>\n        <li>Developed and managed <em>Airflow DAGs</em> in Google Cloud Composer using Python for orchestrating cloud jobs.</li>\n        <li>Engaged in processing healthcare data, crafting efficient and effective queries for BigQuery.</li>\n        <li>Formulated Data Quality Rules to guarantee the cleanliness and integrity of ingested data.</li>\n        <li>Played a pivotal role in assisting and training new team members, leveraging expertise to guide project success.</li>\n    </ul>\n    <h4>Technologies and Tools Used</h4>\n    <ul>\n        <li>Google Cloud Platform</li>\n        <li>Infoworks</li>\n        <li>Apache Airflow</li>\n        <li>Google Cloud Composer</li>\n        <li>BigQuery</li>\n    </ul>",
        "github_link": "N/A"
    },
    {
        "title": "AbInitio Software Upgrade Project",
        "description": "<p><strong>Project Overview:</strong> This project focused on upgrading the AbInitio Software to its latest version, involving comprehensive testing and enhancement of data pipelines and job scripts.</p>\n    <h3>Key Responsibilities</h3>\n    <ul>\n        <li>Collaborated with the support team to monitor and ensure a smooth upgrade process.</li>\n        <li>Conducted extensive testing of AbInitio data pipeline graphs to verify their performance post-upgrade.</li>\n        <li>Rewrote and updated job scripts, ensuring their compatibility and efficiency in calling jobs in the new software environment.</li>\n        <li>Supported and monitored the upgrade of the Control-M Scheduling tool, integral to job scheduling and management.</li>\n        <li>Created new data pipeline jobs, testing and evaluating new features for potential company-wide utilization.</li>\n        <li>Developed detailed unit testing documentation to thoroughly record and showcase all testing procedures and outcomes.</li>\n    </ul>\n    <h4>Technologies and Tools Used</h4>\n    <ul>\n        <li>AbInitio Software</li>\n        <li>Control-M Scheduling Tool</li>\n    </ul>",
        "github_link": "N/A"
    },
    {
        "title": "Collaboration Station Game Education Materials",
        "description": "<p><strong>Project Overview:</strong> Worked in a small team under the guidance of a professor to develop \"Collaboration Station,\" an educational game aimed at teaching young students about the International Space Station.</p>\n    <h3>Key Responsibilities</h3>\n    <ul>\n        <li>Played an active role in the initial design phase of \"Collaboration Station,\" contributing to creative and functional aspects.</li>\n        <li>Developed comprehensive educational materials aligned with state standards to assist teachers in effectively utilizing the game as a learning tool.</li>\n        <li>Contributed to the game's development using Unreal Engine, implementing features and functionalities with C++ coding.</li>\n    </ul>\n    <h4>Technologies and Tools Used</h4>\n    <ul>\n        <li>Unreal Engine</li>\n        <li>C++ Programming Language</li>\n    </ul>",
        "github_link": "N/A"
    },
    {
        "title": "French Lick Resort Application Development Project",
        "description": "<p><strong>Project Overview:</strong> Collaborated with the head programmer at French Lick Resort to develop and maintain various applications, enhancing operational efficiency and user experience.</p>\n    <h3>Key Responsibilities</h3>\n    <ul>\n        <li>Assisted in maintaining and updating the resort's equipment database, significantly improving the website's loading time by 30 seconds to a minute.</li>\n        <li>Developed a Comp Request form for the casino, involving the creation of a SQL database and the construction of the corresponding website using C# and ASP.NET.</li>\n        <li>Operated within the casino environment, adhering to strict casino-level security procedures and holding an L2 - Gaming License.</li>\n    </ul>\n    <h4>Technologies and Tools Used</h4>\n    <ul>\n        <li>SQL Database Management</li>\n        <li>C# Programming Language</li>\n        <li>ASP.NET Framework</li>\n    </ul>\n",
        "github_link": "N/A"
    }
]

